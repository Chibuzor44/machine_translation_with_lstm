{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from tqdm import tqdm, tqdm_gui\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng, fr = [], []\n",
    "with open('../../../Downloads/fra-eng/fra.txt', 'r') as file:\n",
    "    for line in file.readlines():\n",
    "        lines = line.split('\\t')[:2]\n",
    "        eng.append(lines[0])\n",
    "        fr.append(' '.join(lines[1].split(\"\\u202f\")))\n",
    "\n",
    "        \n",
    "# Convert to pandas dataframe\n",
    "data = pd.DataFrame({'english': eng, 'french': fr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>french</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  english    french\n",
       "0     Go.      Va !\n",
       "1     Hi.   Salut !\n",
       "2     Hi.    Salut.\n",
       "3    Run!   Cours !\n",
       "4    Run!  Courez !"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175623, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_en_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"'s\", \"\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"' \", \" \", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]$%&^_\\\\\", \" \", text)\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_symb(text):\n",
    "    for symb in list(\"[-()\\\"#/@;:<>{}`+=~|.!?,]$%&^_\\\\*\"):\n",
    "        text = text.replace(symb, ' ')\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def tagger(text):\n",
    "    \"\"\"\n",
    "    Add <BOS> and <EOS> at beginning and \n",
    "    end of sentence.\n",
    "    \"\"\"\n",
    "    return f'<BOS> {text} <EOS>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text data\n",
    "data.english = data.english.apply(clean_en_text)\n",
    "data.english = data.english.apply(remove_symb)\n",
    "data.french = data.french.apply(remove_symb)\n",
    "data.french = data.french.str.lower()\n",
    "\n",
    "# Apply tagger and insert (<EOS> and <BOS>) to decoder texts\n",
    "data.french = data.french.apply(tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data\n",
    "We will be reserving the last 1000 rows for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en, test_en, train_fr, test_fr = train_test_split(data.english.values,\n",
    "                                                      data.french.values,\n",
    "                                                      test_size=0.001,\n",
    "                                                      random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(176, 176)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_en), len(test_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizers\n",
    "tokenizer_en = Tokenizer(num_words=70000)\n",
    "tokenizer_fr = Tokenizer(num_words=70000)\n",
    "\n",
    "# Fit tokenizer\n",
    "tokenizer_en.fit_on_texts(train_en)\n",
    "tokenizer_fr.fit_on_texts(train_fr)\n",
    "\n",
    "# Create word and index dictionary for english\n",
    "en_word_to_int = tokenizer_en.word_index\n",
    "int_to_en_word = tokenizer_en.index_word\n",
    "\n",
    "# Create word and index dictionary for french\n",
    "fr_word_to_int = tokenizer_fr.word_index\n",
    "int_to_fr_word = tokenizer_fr.index_word\n",
    "\n",
    "# Add BOS and EOS to encoder vocabolary\n",
    "max_indx = sorted(int_to_fr_word)[-1]\n",
    "fr_word_to_int['<BOS>'] = max_indx + 1\n",
    "fr_word_to_int['<EOS>'] = max_indx + 2\n",
    "\n",
    "int_to_fr_word[max_indx + 1] = '<BOS>'\n",
    "int_to_fr_word[max_indx + 2] = '<EOS>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary word list\n",
    "vocab_list_en = list(en_word_to_int.keys())\n",
    "vocab_list_fr = list(fr_word_to_int.keys())\n",
    "\n",
    "# Set parameters\n",
    "vocab_size_en = len(vocab_list_en) + 1\n",
    "vocab_size_fr = len(vocab_list_fr) + 1\n",
    "embedding_dim = 100\n",
    "samples_size = len(train_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname, vocab):\n",
    "    \"\"\"\n",
    "    Loads fasttext embedding and return only words and vectors \n",
    "    in the vocabulary list.\n",
    "    \"\"\"\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    dataset = {}\n",
    "    for line in tqdm(fin):\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        if tokens[0] in vocab:\n",
    "            dataset[tokens[0]] = list(map(float, tokens[1:embedding_dim + 1]))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Load embedding vectors\n",
    "# data_en = load_vectors('../../../Downloads/cc.en.300.vec', vocab_list_en)\n",
    "# print('English is done.')\n",
    "# data_fr = load_vectors('../../../Downloads/cc.fr.300.vec', vocab_list_fr)\n",
    "# print('French is done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save embedded dictionary\n",
    "# with open('./Data/Embeddings/fast_text_french_word_vec.pickle', 'wb') as f:\n",
    "#     pickle.dump(data_fr, f)\n",
    "\n",
    "# with open('./Data/Embeddings/fast_text_english_word_vec.pickle', 'wb') as f:\n",
    "#     pickle.dump(data_en, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedded dictionary\n",
    "with open('./Data/Embeddings/fast_text_french_word_vec.pickle', 'rb') as p:\n",
    "    embed_vec_fr = pickle.load(p)\n",
    "\n",
    "\n",
    "with open('./Data/Embeddings/fast_text_english_word_vec.pickle', 'rb') as p:\n",
    "    embed_vec_en = pickle.load(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix_en = np.zeros((vocab_size_en, embedding_dim))\n",
    "embedding_matrix_fr = np.zeros((vocab_size_fr, embedding_dim))\n",
    "\n",
    "for en_word, i in en_word_to_int.items():\n",
    "    if en_word in embed_vec_en:\n",
    "        embedding_matrix_en[i] = embed_vec_en[en_word]\n",
    "    \n",
    "    \n",
    "for fr_word, j in fr_word_to_int.items():\n",
    "    if fr_word in embed_vec_fr:\n",
    "        embedding_matrix_fr[j] = embed_vec_fr[fr_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximun length of sententence\n",
    "max_len_en = max(len(seq.split()) for seq in train_en)\n",
    "max_len_fr = max(len(seq.split()) for seq in train_fr)\n",
    "\n",
    "# max_len_en = max(train.english.apply(lambda x: len(x.split())))\n",
    "# max_len_fr = max(train.french.apply(lambda x: len(x.split())))\n",
    "\n",
    "# # Inputs\n",
    "# encoder_inputs = tokenizer_en.texts_to_sequences(corpus_en)\n",
    "# decoder_inputs = tokenizer_fr.texts_to_sequences(corpus_fr)\n",
    "\n",
    "# # Pad Sentence\n",
    "# encoder_inputs = pad_sequences(encoder_inputs, padding='post', maxlen=max_len_en)\n",
    "# decoder_inputs = pad_sequences(decoder_inputs, padding='post', maxlen=max_len_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initiate numpy arrays to hold the data that our seq2seq model will use:\n",
    "# encoder_input_data = np.zeros(\n",
    "#                             (samples_size, max_len_en),\n",
    "#                             dtype='float32')\n",
    "# decoder_input_data = np.zeros(\n",
    "#                             (samples_size, max_len_fr),\n",
    "#                             dtype='float32')\n",
    "# decoder_target_data = np.zeros(\n",
    "#                             (samples_size, max_len_fr, vocab_size_fr),\n",
    "#                             dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Process samples, to get input, output, target data:\n",
    "# for i, (input_text, target_text) in tqdm(enumerate(zip(corpus_en, corpus_fr))):\n",
    "# #     print(input_text)\n",
    "#     for t, word in enumerate(input_text.split()):\n",
    "        \n",
    "#         encoder_input_data[i, t] = en_word_to_int[word]\n",
    "        \n",
    "#     for t, word in enumerate(target_text.split()):\n",
    "#         # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "#         decoder_input_data[i, t] = fr_word_to_int[word]\n",
    "#         if t > 0:\n",
    "#             # decoder_target_data will be ahead by one timestep\n",
    "#             # and will not include the start word.\n",
    "#             decoder_target_data[i, t - 1, fr_word_to_int[word]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing training and validation\n",
    "X_train = train_en[:-1000]\n",
    "y_train = train_fr[:-1000]\n",
    "val_en = train_en[-1000:]\n",
    "val_fr = train_fr[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_batch_data(X, y, batch):\n",
    "    \n",
    "    while True:\n",
    "        for j in range(0, len(X), batch):\n",
    "            # initiate numpy arrays to hold the data that our seq2seq model will use:\n",
    "            encoder_input_data1 = np.zeros((batch, max_len_en),\n",
    "                                        dtype='float32')\n",
    "            decoder_input_data1 = np.zeros((batch, max_len_fr),\n",
    "                                        dtype='float32')\n",
    "            decoder_target_data1 = np.zeros((batch, max_len_fr, vocab_size_fr),\n",
    "                                        dtype='float32')\n",
    "\n",
    "            # Process samples, to get input, output, target data:\n",
    "            for i, (input_text1, target_text1) in enumerate(zip(X[j:j+batch], y[j:j+batch])):\n",
    "            #  print(input_text)\n",
    "                for t, word in enumerate(input_text1.split()):\n",
    "\n",
    "                    encoder_input_data1[i, t] = en_word_to_int[word]\n",
    "\n",
    "                for t, word in enumerate(target_text1.split()):\n",
    "                    # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "                    decoder_input_data1[i, t] = fr_word_to_int[word]\n",
    "                    if t > 0:\n",
    "                        # decoder_target_data will be ahead by one timestep\n",
    "                        # and will not include the start word.\n",
    "                        decoder_target_data1[i, t - 1, fr_word_to_int[word]] = 1.\n",
    "                        \n",
    "            yield([encoder_input_data1, decoder_input_data1], decoder_target_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffled_data.english[shuffled_data.english.str.contains('offs')].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.english[train.english.str.contains('offs')].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.english[test.english.str.contains('offs')]#.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "num_units = 128 \n",
    "batch_size = 100\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embedding_layer = Embedding(vocab_size_en, embedding_dim,\n",
    "                               weights=[embedding_matrix_en],\n",
    "                               input_length=max_len_en,\n",
    "                               trainable=False)\n",
    "\n",
    "# encoder_embedding(encoder_embedding_inputs)\n",
    "encoder_embedding_inputs = encoder_embedding_layer(encoder_inputs)\n",
    "\n",
    "\n",
    "encoder = LSTM(num_units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embedding_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding_layer = Embedding(vocab_size_fr, embedding_dim,\n",
    "                               weights=[embedding_matrix_fr],\n",
    "                               input_length=max_len_fr,\n",
    "                               trainable=False)\n",
    "\n",
    "decoder_embedding_inputs = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the \n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder = LSTM(num_units, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder(decoder_embedding_inputs,\n",
    "                                initial_state=encoder_states)\n",
    "decoder_dense = Dense(vocab_size_fr, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 100)    1410900     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    2729800     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 128), (None, 117248      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 128),  117248      embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 27298)  3521442     lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 7,896,638\n",
      "Trainable params: 3,755,938\n",
      "Non-trainable params: 4,140,700\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_samples = test_en.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " 109/1754 [>.............................] - ETA: 4:14:43 - loss: 0.6073"
     ]
    }
   ],
   "source": [
    "history=model.fit_generator(generator=extract_batch_data(X_train, y_train, batch=batch_size),\n",
    "                    steps_per_epoch=samples_size//batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=extract_batch_data(val_en, val_fr, batch=batch_size),\n",
    "                    validation_steps = val_samples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save models\n",
    "# model.save('./Data/models/model.h5')\n",
    "# model.save_weights('./Data/models/weight.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model training\n",
    "# model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder inference model\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder inference model\n",
    "decoder_state_input_h = Input(shape=(num_units,))\n",
    "decoder_state_input_c = Input(shape=(num_units,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder(\n",
    "                                    decoder_embedding_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
    "                        [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, max_decoder_seq_length=70):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    # Populate the first word of target sequence with the BOS.\n",
    "    target_seq[0, 0] = fr_word_to_int['<BOS>']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = int_to_fr_word[sampled_token_index]\n",
    "        decoded_sentence += ' ' + sampled_word\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_word == '<EOS>' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence as per data =>> would it be ethical to sacrifice one person to save many\n",
      "Actual French Translation as per data =>> serait il moral de sacrifier une personne pour en sauver plusieurs <\n",
      "Predicted French Translation predicted by model =>>  il pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas\n"
     ]
    }
   ],
   "source": [
    "test_data_sequence = extract_batch_data(test_en, test_fr, batch=1)\n",
    "k=-1\n",
    "#Adam\n",
    "k+=1\n",
    "(input_seq, actual_output), _ = next(test_data_sequence)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence as per data =>>', test_en[k:k+1][0])\n",
    "print('Actual French Translation as per data =>>', test_fr[k:k+1][0][6:-4])\n",
    "print('Predicted French Translation predicted by model =>>', decoded_sentence[:-4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
