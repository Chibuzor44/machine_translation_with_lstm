{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from tqdm import tqdm, tqdm_gui\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng, fr = [], []\n",
    "with open('../../../Downloads/fra-eng/fra.txt', 'r') as file:\n",
    "    for line in file.readlines():\n",
    "        lines = line.split('\\t')[:2]\n",
    "        eng.append(lines[0])\n",
    "        fr.append(' '.join(lines[1].split(\"\\u202f\")))\n",
    "\n",
    "        \n",
    "# Convert to pandas dataframe\n",
    "data = pd.DataFrame({'english': eng, 'french': fr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>french</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  english    french\n",
       "0     Go.      Va !\n",
       "1     Hi.   Salut !\n",
       "2     Hi.    Salut.\n",
       "3    Run!   Cours !\n",
       "4    Run!  Courez !"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175623, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_en_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"'s\", \"\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"' \", \" \", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]$%&^_\\\\\", \" \", text)\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_symb(text):\n",
    "    for symb in list(\"[-()\\\"#/@;:<>{}`+=~|.!?,]$%&^_\\\\*\"):\n",
    "        text = text.replace(symb, ' ')\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def tagger(text):\n",
    "    \"\"\"\n",
    "    Add <BOS> and <EOS> at beginning and \n",
    "    end of sentence.\n",
    "    \"\"\"\n",
    "    return f'<BOS> {text} <EOS>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text data\n",
    "data.english = data.english.apply(clean_en_text)\n",
    "data.english = data.english.apply(remove_symb)\n",
    "data.french = data.french.apply(remove_symb)\n",
    "data.french = data.french.str.lower()\n",
    "\n",
    "# Apply tagger and insert (<EOS> and <BOS>) to decoder texts\n",
    "data.french = data.french.apply(tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data\n",
    "We will be reserving the last 1000 rows for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_data = data.sample(frac=1, random_state=random_state).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>french</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>would it be ethical to sacrifice one person to...</td>\n",
       "      <td>&lt;BOS&gt; serait il moral de sacrifier une personn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>we will sort this out tomorrow</td>\n",
       "      <td>&lt;BOS&gt; nous réglerons cela demain &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fur coats are on sale</td>\n",
       "      <td>&lt;BOS&gt; les manteaux de fourrure sont en promoti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i have never told anyone that before</td>\n",
       "      <td>&lt;BOS&gt; je n'ai jamais dit cela à quiconque aupa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i must warn them</td>\n",
       "      <td>&lt;BOS&gt; il me faut les prévenir &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0  would it be ethical to sacrifice one person to...   \n",
       "1                     we will sort this out tomorrow   \n",
       "2                              fur coats are on sale   \n",
       "3               i have never told anyone that before   \n",
       "4                                   i must warn them   \n",
       "\n",
       "                                              french  \n",
       "0  <BOS> serait il moral de sacrifier une personn...  \n",
       "1             <BOS> nous réglerons cela demain <EOS>  \n",
       "2  <BOS> les manteaux de fourrure sont en promoti...  \n",
       "3  <BOS> je n'ai jamais dit cela à quiconque aupa...  \n",
       "4                <BOS> il me faut les prévenir <EOS>  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = shuffled_data.iloc[:-1000]\n",
    "test = shuffled_data.iloc[-1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract corpus\n",
    "corpus_en = train.english.values\n",
    "corpus_fr = train.french.values\n",
    "\n",
    "# Initialize tokenizers\n",
    "tokenizer_en = Tokenizer(num_words=70000)\n",
    "tokenizer_fr = Tokenizer(num_words=70000)\n",
    "\n",
    "# Fit tokenizer\n",
    "tokenizer_en.fit_on_texts(corpus_en)\n",
    "tokenizer_fr.fit_on_texts(corpus_fr)\n",
    "\n",
    "# Create word and index dictionary for english\n",
    "en_word_to_int = tokenizer_en.word_index\n",
    "int_to_en_word = tokenizer_en.index_word\n",
    "\n",
    "# Create word and index dictionary for french\n",
    "fr_word_to_int = tokenizer_fr.word_index\n",
    "int_to_fr_word = tokenizer_fr.index_word\n",
    "\n",
    "# Add BOS and EOS to encoder vocabolary\n",
    "max_indx = sorted(int_to_fr_word)[-1]\n",
    "fr_word_to_int['<BOS>'] = max_indx + 1\n",
    "fr_word_to_int['<EOS>'] = max_indx + 2\n",
    "\n",
    "int_to_fr_word[max_indx + 1] = '<BOS>'\n",
    "int_to_fr_word[max_indx + 2] = '<EOS>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary word list\n",
    "vocab_list_en = list(en_word_to_int.keys())\n",
    "vocab_list_fr = list(fr_word_to_int.keys())\n",
    "\n",
    "# Set parameters\n",
    "vocab_size_en = len(vocab_list_en) + 1\n",
    "vocab_size_fr = len(vocab_list_fr) + 1\n",
    "embedding_dim = 100\n",
    "samples_size = train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname, vocab):\n",
    "    \"\"\"\n",
    "    Loads fasttext embedding and return only words and vectors \n",
    "    in the vocabulary list.\n",
    "    \"\"\"\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    dataset = {}\n",
    "    for line in tqdm(fin):\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        if tokens[0] in vocab:\n",
    "            dataset[tokens[0]] = list(map(float, tokens[1:embedding_dim + 1]))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load embedding vectors\n",
    "# data_en = load_vectors('../../../Downloads/cc.en.300.vec', vocab_list_en)\n",
    "# print('English is done.')\n",
    "# data_fr = load_vectors('../../../Downloads/cc.fr.300.vec', vocab_list_fr)\n",
    "# print('French is done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save embedded dictionary\n",
    "# with open('./Data/Embeddings/fast_text_french_word_vec.pickle', 'wb') as f:\n",
    "#     pickle.dump(data_fr, f)\n",
    "\n",
    "# with open('./Data/Embeddings/fast_text_english_word_vec.pickle', 'wb') as f:\n",
    "#     pickle.dump(data_en, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedded dictionary\n",
    "with open('./Data/Embeddings/fast_text_french_word_vec.pickle', 'rb') as p:\n",
    "    embed_vec_fr = pickle.load(p)\n",
    "\n",
    "\n",
    "with open('./Data/Embeddings/fast_text_english_word_vec.pickle', 'rb') as p:\n",
    "    embed_vec_en = pickle.load(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix_en = np.zeros((vocab_size_en, embedding_dim))\n",
    "embedding_matrix_fr = np.zeros((vocab_size_fr, embedding_dim))\n",
    "\n",
    "for en_word, i in en_word_to_int.items():\n",
    "    if en_word in embed_vec_en:\n",
    "        embedding_matrix_en[i] = embed_vec_en[en_word]\n",
    "    \n",
    "    \n",
    "for fr_word, j in fr_word_to_int.items():\n",
    "    if fr_word in embed_vec_fr:\n",
    "        embedding_matrix_fr[j] = embed_vec_fr[fr_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximun length of sententence\n",
    "max_len_en = max(train.english.apply(lambda x: len(x.split())))\n",
    "max_len_fr = max(train.french.apply(lambda x: len(x.split())))\n",
    "\n",
    "# # Inputs\n",
    "# encoder_inputs = tokenizer_en.texts_to_sequences(corpus_en)\n",
    "# decoder_inputs = tokenizer_fr.texts_to_sequences(corpus_fr)\n",
    "\n",
    "# # Pad Sentence\n",
    "# encoder_inputs = pad_sequences(encoder_inputs, padding='post', maxlen=max_len_en)\n",
    "# decoder_inputs = pad_sequences(decoder_inputs, padding='post', maxlen=max_len_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate numpy arrays to hold the data that our seq2seq model will use:\n",
    "encoder_input_data = np.zeros(\n",
    "                            (samples_size, max_len_en),\n",
    "                            dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "                            (samples_size, max_len_fr),\n",
    "                            dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "                            (samples_size, max_len_fr, vocab_size_fr),\n",
    "                            dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "174623it [00:08, 20156.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process samples, to get input, output, target data:\n",
    "for i, (input_text, target_text) in tqdm(enumerate(zip(corpus_en, corpus_fr))):\n",
    "#     print(input_text)\n",
    "    for t, word in enumerate(input_text.split()):\n",
    "        \n",
    "        encoder_input_data[i, t] = en_word_to_int[word]\n",
    "        \n",
    "    for t, word in enumerate(target_text.split()):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t] = fr_word_to_int[word]\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start word.\n",
    "            decoder_target_data[i, t - 1, tokenizer_fr.word_index[word]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "num_units = 128 \n",
    "batch_size = 100\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embedding_inputs = Embedding(vocab_size_en, embedding_dim,\n",
    "                               weights=[embedding_matrix_en],\n",
    "                               input_length=max_len_en,\n",
    "                               trainable=False)(encoder_inputs)\n",
    "\n",
    "# encoder_embedding(encoder_embedding_inputs)\n",
    "encoder = LSTM(num_units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embedding_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding = Embedding(vocab_size_fr, embedding_dim,\n",
    "                               weights=[embedding_matrix_fr],\n",
    "                               input_length=max_len_fr,\n",
    "                               trainable=False)(decoder_inputs)\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the \n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder = LSTM(num_units, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder(decoder_embedding,\n",
    "                                initial_state=encoder_states)\n",
    "decoder_dense = Dense(vocab_size_fr, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 100)    1407600     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    2725200     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 128), (None, 117248      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 128),  117248      embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 27252)  3515508     lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 7,882,804\n",
      "Trainable params: 3,750,004\n",
      "Non-trainable params: 4,132,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder inference model\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Dencoder inference model\n",
    "decoder_state_input_h = Input(shape=(num_units,))\n",
    "decoder_state_input_c = Input(shape=(num_units,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder(\n",
    "                                    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
    "                        [decoder_outputs] + decoder_states)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
