{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import io, re\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from tqdm import tqdm, tqdm_gui\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./Data/prepped/train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_text</th>\n",
       "      <th>french_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Debates of the Senate (Hansard)</td>\n",
       "      <td>dèbats du sènat (hansard)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1st Session, 36th Parliament,</td>\n",
       "      <td>1ëre session, 36e lègislature,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Volume 137, Issue 1</td>\n",
       "      <td>volume 137, numèro 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Monday, September 22, 1997</td>\n",
       "      <td>le lundi 22 septembre 1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Honourable Gildas L. Molgat, Speaker</td>\n",
       "      <td>l'honorable gildas l. molgat, prèsident</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               english_text  \\\n",
       "0           Debates of the Senate (Hansard)   \n",
       "1             1st Session, 36th Parliament,   \n",
       "2                       Volume 137, Issue 1   \n",
       "3                Monday, September 22, 1997   \n",
       "4  The Honourable Gildas L. Molgat, Speaker   \n",
       "\n",
       "                               french_text  \n",
       "0                dèbats du sènat (hansard)  \n",
       "1           1ëre session, 36e lègislature,  \n",
       "2                     volume 137, numèro 1  \n",
       "3               le lundi 22 septembre 1997  \n",
       "4  l'honorable gildas l. molgat, prèsident  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train_data.copy()\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocess\n",
    "I will be applying the following steps in the preprocessing of the data\n",
    "1. Text Cleaning\n",
    "2. Put `<BOS>` tag and `<EOS>` tag for decoder input\n",
    "3. Make Vocabulary (VOCAB_SIZE)\n",
    "4. Tokenize Bag of words to Bag of IDs\n",
    "5. Padding (MAX_LEN)\n",
    "6. Word Embedding (EMBEDDING_DIM)\n",
    "7. Reshape the Data depends on neural network shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_en_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"'s\", \"\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"' \", \" \", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]$%&^_\\\\\", \" \", text)\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_symb(text):\n",
    "    for symb in list(\"[-()\\\"#/@;:<>{}`+=~|.!?,]$%&^_\\\\*\"):\n",
    "        text = text.replace(symb, ' ')\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def tagger(text):\n",
    "    \"\"\"\n",
    "    Add <BOS> and <EOS> at beginning and \n",
    "    end of sentence.\n",
    "    \"\"\"\n",
    "    return f'<BOS> {text} <EOS>'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<BOS> the president motorcade <EOS>',\n",
       " '<BOS> i cannot because you do not <EOS>',\n",
       " '<BOS> i have heard, that is why i am here <EOS>',\n",
       " '<BOS> they are coming <EOS>',\n",
       " '<BOS> where is the house, i will live there <EOS>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tezt functions\n",
    "fake_sent = [\"The President's motorcade\", \"I can't because you don't\", \"I've heard, that's why I'm here\",\n",
    "            \"They're coming\", \"Where's the house, I'll live there\"]\n",
    "\n",
    "[tagger(clean_en_text(sent)) for sent in fake_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply text cleaning function to english text\n",
    "train.english_text = train.english_text.apply(clean_en_text)\n",
    "train.english_text = train.english_text.apply(remove_symb)\n",
    "train.french_text = train.french_text.apply(remove_symb)\n",
    "\n",
    "# Apply tagger and insert (<EOS> and <BOS>) to decoder texts\n",
    "train.french_text = train.french_text.apply(tagger)\n",
    "\n",
    "# Extract corpus\n",
    "corpus_fr = train.french_text.values\n",
    "corpus_en = train.english_text.values\n",
    "\n",
    "# Initialize tokenizers\n",
    "tokenizer_en = Tokenizer(num_words=70000)\n",
    "tokenizer_fr = Tokenizer(num_words=70000)\n",
    "\n",
    "# Fit tokenizer\n",
    "tokenizer_en.fit_on_texts(corpus_en)\n",
    "tokenizer_fr.fit_on_texts(corpus_fr)\n",
    "\n",
    "# Create word and index dictionary\n",
    "en_word_to_int = tokenizer_en.word_index\n",
    "int_to_en_word = tokenizer_en.index_word\n",
    "\n",
    "fr_word_to_int = tokenizer_fr.word_index\n",
    "int_to_fr_word = tokenizer_en.index_word\n",
    "\n",
    "\n",
    "# Add BOS and EOS to encoder vocabolary\n",
    "max_indx = sorted(int_to_fr_word)[-1]\n",
    "fr_word_to_int['<BOS>'] = max_indx + 1\n",
    "fr_word_to_int['<EOS>'] = max_indx + 2\n",
    "\n",
    "int_to_fr_word[max_indx + 1] = '<BOS>'\n",
    "int_to_fr_word[max_indx + 2] = '<EOS>'\n",
    "\n",
    "\n",
    "# Create vocabulary word list\n",
    "vocab_list_en = list(en_word_to_int.keys())\n",
    "vocab_list_fr = list(fr_word_to_int.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "vocab_size_en = len(vocab_list_en) + 1\n",
    "vocab_size_fr = len(vocab_list_fr) + 1\n",
    "embedding_dim = 100\n",
    "samples_size = train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname, vocab):\n",
    "    \"\"\"\n",
    "    Loads fasttext embedding and return only words and vectors \n",
    "    in the vocabulary list.\n",
    "    \"\"\"\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in tqdm(fin):\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        if tokens[0] in vocab:\n",
    "            data[tokens[0]] = list(map(float, tokens[1:embedding_dim + 1]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load embedding vectors\n",
    "# data_en = load_vectors('../../../Downloads/cc.en.300.vec', vocab_list_en)\n",
    "# print('English is done.')\n",
    "# data_fr = load_vectors('../../../Downloads/cc.fr.300.vec', vocab_list_fr)\n",
    "# print('French is done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save embedded dictionary\n",
    "# with open('./Data/word_embedding/fast_text_french_word_vec.pickle', 'wb') as f:\n",
    "#     pickle.dump(data_fr, f)\n",
    "\n",
    "# with open('./Data/word_embedding/fast_text_english_word_vec.pickle', 'wb') as f:\n",
    "#     pickle.dump(data_en, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedded dictionary\n",
    "with open('./Data/word_embedding/fast_text_french_word_vec.pickle', 'rb') as p:\n",
    "    embed_vec_fr = pickle.load(p)\n",
    "\n",
    "\n",
    "with open('./Data/word_embedding/fast_text_english_word_vec.pickle', 'rb') as p:\n",
    "    embed_vec_en = pickle.load(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix_en = np.zeros((vocab_size_en, embedding_dim))\n",
    "embedding_matrix_fr = np.zeros((vocab_size_fr, embedding_dim))\n",
    "\n",
    "for en_word, i in en_word_to_int.items():\n",
    "    if en_word in embed_vec_en:\n",
    "        embedding_matrix_en[i] = embed_vec_en[en_word]\n",
    "    \n",
    "    \n",
    "for fr_word, j in fr_word_to_int.items():\n",
    "    if fr_word in embed_vec_fr:\n",
    "        embedding_matrix_fr[j] = embed_vec_fr[fr_word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximun length of sententence\n",
    "max_len_en = max(train.english_text.apply(lambda x: len(x.split())))\n",
    "max_len_fr = max(train.french_text.apply(lambda x: len(x.split())))\n",
    "\n",
    "# # Inputs\n",
    "# encoder_inputs = tokenizer_en.texts_to_sequences(corpus_en)\n",
    "# decoder_inputs = tokenizer_fr.texts_to_sequences(corpus_fr)\n",
    "\n",
    "# # Pad Sentence\n",
    "# encoder_inputs = pad_sequences(encoder_inputs, padding='post', maxlen=max_len_en)\n",
    "# decoder_inputs = pad_sequences(decoder_inputs, padding='post', maxlen=max_len_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate numpy arrays to hold the data that our seq2seq model will use:\n",
    "encoder_input_data = np.zeros(\n",
    "                            (samples_size, max_len_en),\n",
    "                            dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "                            (samples_size, max_len_fr),\n",
    "                            dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "                            (samples_size, max_len_fr, vocab_size_fr),\n",
    "                            dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "182135it [00:18, 9747.11it/s] \n"
     ]
    }
   ],
   "source": [
    "# Process samples, to get input, output, target data:\n",
    "for i, (input_text, target_text) in tqdm(enumerate(zip(corpus_en, corpus_fr))):\n",
    "#     print(input_text)\n",
    "    for t, word in enumerate(input_text.split()):\n",
    "        \n",
    "        encoder_input_data[i, t] = en_word_to_int[word]\n",
    "        \n",
    "    for t, word in enumerate(target_text.split()):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t] = fr_word_to_int[word]\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start word.\n",
    "            decoder_target_data[i, t - 1, tokenizer_fr.word_index[word]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "num_units = 128 \n",
    "batch_size = 100\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embedding_inputs = Embedding(vocab_size_en, embedding_dim,\n",
    "                               weights=[embedding_matrix_en],\n",
    "                               input_length=max_len_en,\n",
    "                               trainable=False)(encoder_inputs)\n",
    "\n",
    "# encoder_embedding(encoder_embedding_inputs)\n",
    "encoder = LSTM(num_units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embedding_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding = Embedding(vocab_size_fr, embedding_dim,\n",
    "                               weights=[embedding_matrix_fr],\n",
    "                               input_length=max_len_fr,\n",
    "                               trainable=False)(decoder_inputs)\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the \n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder = LSTM(num_units, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder(decoder_embedding,\n",
    "                                initial_state=encoder_states)\n",
    "decoder_dense = Dense(vocab_size_fr, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder inference model\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Dencoder inference model\n",
    "decoder_state_input_h = Input(shape=(num_units,))\n",
    "decoder_state_input_c = Input(shape=(num_units,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder(\n",
    "                                    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
    "                        [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
